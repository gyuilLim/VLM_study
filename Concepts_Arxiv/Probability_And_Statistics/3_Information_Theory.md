# Information Theory(정보 이론)

정보 이론이란 정보를 수학적으로 정의하고 측정하는 방법을 연구하는 학문이다.



머신 러닝은 데이터 정보를 다루기 때문에 정보 이론의 개념이 중요하다. (실제로 loss 함수에서 엔트로피 개념이 쓰임)

 ---

 # Information Content(정보량)

정보량이란 정보이론의 기본 단위이며, 어떤 사건이 발생했을 때 얻는 정보의 수치이다.

이산 확률 변수 $x$가 있다고 하자. 이 때, 이 $x$가 가지고 있는 정보량이라는게 있다.

정보량은 ‘놀라움의 정도’라고도 하는데, 일어날 가능성이 낮은 사건이 발생했을 때의 정보량이 더 크기(더 놀랍기) 때문이다.

즉 정보량은 $x$가 발생할 확률에 반비례하며 따라서 확률 분포 $p(x)$에 종속된다고 할 수 있다.

## 정보량을 표현하는 함수

정보량을 표현하는 어떤 함수 h(x)가 있다고 하자.

독립적인 두 사건 $x,y$가 있을 때, 이 두 사건이 동시에 발생한 경우 정보량의 합은 각각의 사건이 따로 발생했을 때의 정보량을 더한 것과 같을 것이다. 즉 $h(x,y)=h(x)+h(y)$이다.

마찬가지로 사건들이 독립적이기 때문에, $p(x,y)=p(x)p(y)$라고 할 수 있고 이 관계에서 $h(x)$는 $p(x)$의 로그에 해당함을 알 수 있다. 따라서 

$$h(x)=−\log_2p(x)$$

로 관계를 정의할 수 있다. ($p(x)$는 이산 확률 변수의 확률 분포이기 때문에 범위가 0과 1사이)

$h(x)$를 양수로 취급하기 위해 -가 붙는다. 따라서 사건 발생 확률이 0에 가까울수록 무한대, 1이면 0이다.(놀라움의 정도와 같은 이치)

로그의 밑 2는 정보 이론 학계의 관습이라고 한다. 정보량의 단위는 비트(Bit)이기 때문에, 이진수를 위한 편의이다.

## Entropy(엔트로피)

정보 이론에서 엔트로피는 정보량의 기댓값이다. 즉 어떤 사건 $x$에 대한 확률 분포 $p(x)$가 가지고있는 평균 정보량을 의미한다.

따라서 평균 정보량은 $h(x)$그래프의 넓이인 셈이다. 그래프의 넓이를 구하려면 함수를 적분하면 된지만 $x$는 이산 확률 변수이므로 엔트로피는

$$H[x]=−\sum p(x)\log_2p(x)$$

이렇게 표현할 수 있다.

엔트로피가 높은 경우는 동시에 정보량의 기댓값이 높다는 것이고 이는 사건 $x$가 발생할 확률이 낮다는 것을 의미한다.

동시에 엔트로피가 낮으면 사건 $x$가 발생할 확률이 높다는 것을 의미한다.

---


# Kullback-Leibelr Divergence(쿨백-라이블러 발산)

두 확률 분포 $P(x)$와 $Q(x)$ 간의 차이를 측정하는 **비대칭적 척도**이며 분포간의 "정보 손실"을 나타낸다.

$$D_{KL}(P||Q) = \sum_x P(x)\log \frac{P(x)}{Q(x)}$$

- $P(x)$ : 실제 데이터 분포
- $Q(x)$ : 모델이 추정한 분포
- $\log\frac{P(x)}{Q(x)}$ : $Q(x)$로 $P(x)$를 설명하기 위한 "비효율성"

$P(x) = Q(x)$ 일 때, $D_{KL}(P||Q)=0$ 으로 최소이다.

두 분포 $P(x)$와 $Q(x)$의 차이가 클수록 $\log \frac{P(x)}{Q(x)}$ 가 커진다.

또한 항상 $D_{KL}(P||Q) \ge 0$ 이다. $Q(x)$가 $P(x)$를 설명하는데 "추가 정보"가 필요하기 때문이다.

따라서 KL Divergence를 모델의 예측 분포와 정답 레이블 분포의 차이를 최소화하는 데에 사용한 것이 Cross Entropy Loss이다.

---

# Cross Entropy Loss(크로스 엔트로피 손실)

두 분포간의 엔트로피를 Cross Entropy(크로스 엔트로피)라고 한다.

머신러닝이나 딥러닝에서는 모델의 예측 분포와 정답 분포의 엔트로피를 감소시키는 방향으로 학습한다.

엔트로피를 감소시킨다. -> 평균 정보량이 작아진다. -> 특정 확률 변수가 발생할 확률이 높아진다. -> 모델의 예측이 정답일 확률이 높아진다.

$$D_{KL}(P||Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}$$

쿨백-라이블러 발산 수식을 풀어쓰면

$$= \sum P(x)\log P(x) - \sum P(x)\log Q(x)$$

로 나타낼 수 있는데, 여기서 $P(x)\log P(x)$는 $P(x)$의 엔트로피이고, $P(x)\log Q(x)$는 $Q(x)$에 대한 $P(x)$의 Cross Entropy이다.

엔트로피로 바꾸어서 위 식을 다시 쓰면,

$$D_{KL}(P||Q) = H(P,Q) - H(P)$$

이다.

$H(P)$는 $P(x)$의 엔트로피인데, $P(x)$가 이미 고정된 분포이므로 상수이기 때문에 크로스 엔트로피 $H(P, Q)$를 최소화하는 것이 $D_{KL}(P||Q)$ 를 감소시키는 것이다.
