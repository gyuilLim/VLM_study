## Categorizing

### Model

CLIP(10) : Learning transferable visual models from natural language supervision(2021)

ALIGN(17) : Scaling up visual and vision-language representation learning with noisy text supervision(2021)

SLIP(64) : Self-supervision meets language-image pretraining(2022)

MoCo(12) : Momentum contrast for unsupervised visual repre-sentation learning(2020)

Coca(19) : Contrastive captioners are image-text foundation models(2022)

LLaVA : Visual Instruction Tuning(2023)

DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning(2025)

CLIP-Adapter: Better Vision-Language Models with Feature Adapters(2021)

BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation(2022)

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models(2023)

CLIPPO : Image-and-language understanding from pixels only(2022)


### Dataset

Conceptual Captions : A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning(2018)


### Learning

InfoNCE Loss(68) : Representation learning with contrastive predictive coding(2018)

**Noise Contrastive Estimation** - NCE Loss : Noise-contrastive estimation: A new estimation principle for unnormalized statistical models(2010)

**Image-text-label contrastive learning** - UniCL : Unified contrastive learning in image-text-label space(2022)

---
