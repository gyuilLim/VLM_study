## Categorizing

### Model

CLIP(10) : Learning transferable visual models from natural language supervision(2021)

ALIGN(17) : Scaling up visual and vision-language representation learning with noisy text supervision(2021)

SLIP(64) : Self-supervision meets language-image pretraining(2022)

MoCo(12) : Momentum contrast for unsupervised visual repre-sentation learning(2020)

Coca(19) : Contrastive captioners are image-text foundation models(2022)

LLaVA : Visual Instruction Tuning(2023)

DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning(2025)

CLIP-Adapter: Better Vision-Language Models with Feature Adapters(2021)

BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation(2022)

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models(2023)

CLIPPO : Image-and-language understanding from pixels only(2022)

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2019)

BEiT: BERT Pre-Training of Image Transformers(2021)

LLaMA: Open and Efficient Foundation Language Models(2023)

CoOp : Learning to prompt for vision-language models(2022)


### Dataset

Conceptual Captions : A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning(2018)


### Learning

InfoNCE Loss(68) : Representation learning with contrastive predictive coding(2018)

**Noise Contrastive Estimation** - NCE Loss : Noise-contrastive estimation: A new estimation principle for unnormalized statistical models(2010)

**Image-text-label contrastive learning** - UniCL : Unified contrastive learning in image-text-label space(2022)

### Evaluation Metric

BLEU: a method for automatic evaluation of machine translation(2002)

METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments(2005)

ROUGE: A Package for Automatic Evaluation of Summaries(2004)

CIDEr: Consensus-based Image Description Evaluation(2014)

---
